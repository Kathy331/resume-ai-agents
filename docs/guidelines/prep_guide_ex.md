> ‚ö†Ô∏è **formatting notice**  
> hyperlinks should be embedded like this: [glassdoor](https://www.glassdoor.com)  
> avoid bold or italic styling unless absolutely necessary.

# interview prep requirements template

## 1. before interview

- email mention to pick time slot between aug, 6, 7, or 8 (all pst)  
- make sure to complete any online assessments 24 hours before interview (if applicable)
- others....(if needed)

## 2. interviewer background

- priya deshmukh is currently a staff software engineer at nebula systems, focused on AI infrastructure.  
- she earned her M.S. in computer science from stanford in 2016.  
- she previously worked at google on the TPU compiler team for 4 years.  
- [linkedin.com/in/priyadeshmukh-ai](https://linkedin.com/in/priyadeshmukh-ai)

## 3. company background

- nebula systems is an AI-first cloud infrastructure startup focused on LLM deployment platforms. their mission is to make scalable AI serving as accessible as deploying a website.  
- in jan 2025, they raised a $120m series C led by sequoia and are reportedly expanding their ML infra team.  
- employee reviews on glassdoor highlight a strong engineering culture but note occasional pressure from fast-paced deadlines.  
  - [glassdoor](https://www.glassdoor.com)  
  - [techcrunch - jan 2025](https://techcrunch.com/example)

## 4. technical preparations

- role: backend-focused ML infrastructure engineer  
- prep areas:
  - leetcode medium/hard questions involving concurrency, caching, and scheduling  
  - system design patterns: model serving, feature store, streaming architecture  
  - hands-on familiarity with: docker, kubernetes, torchserve, triton inference server  
  - concepts to review: a/b testing infra, dynamic batching, grpc performance tuning

## 5. questions to ask

- to interviewer:
  - what drew you to the AI infra space, and what‚Äôs been most rewarding in your time at nebula?
  - how do you see the infra stack evolving over the next 6‚Äì12 months?

- to company:
  - how do you ensure low-latency LLM serving at scale, especially under unpredictable traffic?
  - what metrics does the company use to measure success in infrastructure reliability?

## 6. common questions

- ‚Äúdescribe how you‚Äôd architect a scalable model serving system that can handle dynamic batch sizes.‚Äù  
- ‚Äúhow would you improve latency if inference spikes 10x during peak usage?‚Äù  
- reddit user mentioned being asked about GPU scheduling tradeoffs in production:  
  - [reddit /r/mlops](https://www.reddit.com/r/mlops/comments/example_gpu_serving)



----------------HTML FORMATTING----------------
<div style="font-family: sans-serif; font-size: 16px; line-height: 1.5;">
  <p>üìã Interview Prep Guide<br>
  1. Before Interview:<br>
  - Email mentions to pick a time slot between Aug 6, 7, or 8 (all PST)<br>
  - Make sure to complete any online assessments 24 hours before interview (if applicable)<br>
  - Others... (if needed)<br>
  2. Interviewer Background:<br>
  - Priya Deshmukh is currently a staff software engineer at Nebula Systems, focused on AI infrastructure<br>
  - She earned her M.S. in computer science from Stanford in 2016<br>
  - She previously worked at Google on the TPU compiler team for 4 years<br>
  - <a href="https://linkedin.com/in/priyadeshmukh-ai" target="_blank">linkedin</a><br>
  3. Company Background:<br>
  - Nebula Systems is an AI-first cloud infrastructure startup focused on LLM deployment platforms. Their mission is to make scalable AI serving as accessible as deploying a website<br>
  - In Jan 2025, they raised a $120M Series C led by Sequoia and are reportedly expanding their ML infra team<br>
  - Employee reviews on <a href="https://www.glassdoor.com" target="_blank">glassdoor</a> highlight a strong engineering culture but note occasional pressure from fast-paced deadlines<br>
  - <a href="https://techcrunch.com/example" target="_blank">techcrunch - jan 2025</a><br>
  4. Technical Preparations:<br>
  - Role: backend-focused ML infrastructure engineer<br>
  - Prep areas: LeetCode medium/hard questions involving concurrency, caching, and scheduling<br>
  - System design patterns: model serving, feature store, streaming architecture<br>
  - Hands-on familiarity with: Docker, Kubernetes, TorchServe, Triton Inference Server<br>
  - Concepts to review: A/B testing infra, dynamic batching, gRPC performance tuning<br>
  5. Questions to Ask:<br>
  To interviewer:<br>
  - What drew you to the AI infra space, and what's been most rewarding in your time at Nebula?<br>
  - How do you see the infra stack evolving over the next 6‚Äì12 months?<br>
  To company:<br>
  - How do you ensure low-latency LLM serving at scale, especially under unpredictable traffic?<br>
  - What metrics does the company use to measure success in infrastructure reliability?<br>
  6. Common Questions:<br>
  - "Describe how you'd architect a scalable model serving system that can handle dynamic batch sizes."<br>
  - "How would you improve latency if inference spikes 10x during peak usage?"<br>
  - Reddit user mentioned being asked about GPU scheduling tradeoffs in production: <a href="https://www.reddit.com/r/mlops/comments/example_gpu_serving" target="_blank">reddit /r/mlops</a></p>
</div>