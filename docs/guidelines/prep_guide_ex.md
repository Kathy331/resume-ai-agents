> ‚ö†Ô∏è **formatting notice**  
> hyperlinks should be embedded like this: [glassdoor](https://www.glassdoor.com)  
> avoid bold or italic styling unless absolutely necessary.

# interview prep requirements template

## 1. before interview

- email mention to pick time slot between aug, 6, 7, or 8 (all pst)  
- make sure to complete any online assessments 24 hours before interview (if applicable)
- others....(if needed)

## 2. interviewer background

- priya deshmukh is currently a staff software engineer at nebula systems, focused on AI infrastructure.  
- she earned her M.S. in computer science from stanford in 2016.  
- she previously worked at google on the TPU compiler team for 4 years.  
- [linkedin.com/in/priyadeshmukh-ai](https://linkedin.com/in/priyadeshmukh-ai)

## 3. company background

- nebula systems is an AI-first cloud infrastructure startup focused on LLM deployment platforms. their mission is to make scalable AI serving as accessible as deploying a website.  
- in jan 2025, they raised a $120m series C led by sequoia and are reportedly expanding their ML infra team.  
- employee reviews on glassdoor highlight a strong engineering culture but note occasional pressure from fast-paced deadlines.  
  - [glassdoor](https://www.glassdoor.com)  
  - [techcrunch - jan 2025](https://techcrunch.com/example)

## 4. technical preparations

- role: backend-focused ML infrastructure engineer  
- prep areas:
  - leetcode medium/hard questions involving concurrency, caching, and scheduling  
  - system design patterns: model serving, feature store, streaming architecture  
  - hands-on familiarity with: docker, kubernetes, torchserve, triton inference server  
  - concepts to review: a/b testing infra, dynamic batching, grpc performance tuning

## 5. questions to ask

- to interviewer:
  - what drew you to the AI infra space, and what‚Äôs been most rewarding in your time at nebula?
  - how do you see the infra stack evolving over the next 6‚Äì12 months?

- to company:
  - how do you ensure low-latency LLM serving at scale, especially under unpredictable traffic?
  - what metrics does the company use to measure success in infrastructure reliability?

## 6. common questions

- ‚Äúdescribe how you‚Äôd architect a scalable model serving system that can handle dynamic batch sizes.‚Äù  
- ‚Äúhow would you improve latency if inference spikes 10x during peak usage?‚Äù  
- reddit user mentioned being asked about GPU scheduling tradeoffs in production:  
  - [reddit /r/mlops](https://www.reddit.com/r/mlops/comments/example_gpu_serving)



----------------HTML FORMATTING----------------
<div style="font-family: sans-serif; font-size: 16px; line-height: 1.5;">
  <p>üìã Interview Prep Guide</p>

  <p>1. Before Interview:</p>
  <ul>
    <li>Email mentions to pick a time slot between Aug 6, 7, or 8 (all PST).</li>
    <li>Make sure to complete any online assessments 24 hours before interview (if applicable).</li>
    <li>Others... (if needed).</li>
  </ul>

  <p>2. Interviewer Background:</p>
  <ul>
    <li>Priya Deshmukh is currently a staff software engineer at Nebula Systems, focused on AI infrastructure.</li>
    <li>She earned her M.S. in computer science from Stanford in 2016.</li>
    <li>She previously worked at Google on the TPU compiler team for 4 years.</li>
    <li><a href="https://linkedin.com/in/priyadeshmukh-ai" target="_blank">linkedin</a></li>
  </ul>

  <p>3. Company Background:</p>
  <ul>
    <li>Nebula Systems is an AI-first cloud infrastructure startup focused on LLM deployment platforms. Their mission is to make scalable AI serving as accessible as deploying a website.</li>
    <li>In Jan 2025, they raised a $120M Series C led by Sequoia and are reportedly expanding their ML infra team.</li>
    <li>Employee reviews on <a href="https://www.glassdoor.com" target="_blank">glassdoor</a> highlight a strong engineering culture but note occasional pressure from fast-paced deadlines.</li>
    <li><a href="https://techcrunch.com/example" target="_blank">techcrunch - jan 2025</a></li>
  </ul>

  <p>4. Technical Preparations:</p>
  <ul>
    <li>Role: backend-focused ML infrastructure engineer</li>
    <li>Prep areas:
      <ul>
        <li>LeetCode medium/hard questions involving concurrency, caching, and scheduling</li>
        <li>System design patterns: model serving, feature store, streaming architecture</li>
        <li>Hands-on familiarity with: Docker, Kubernetes, TorchServe, Triton Inference Server</li>
        <li>Concepts to review: A/B testing infra, dynamic batching, gRPC performance tuning</li>
      </ul>
    </li>
  </ul>

  <p>5. Questions to Ask:</p>
  <p>To interviewer:</p>
  <ul>
    <li>What drew you to the AI infra space, and what‚Äôs been most rewarding in your time at Nebula?</li>
    <li>How do you see the infra stack evolving over the next 6‚Äì12 months?</li>
  </ul>
  <p>To company:</p>
  <ul>
    <li>How do you ensure low-latency LLM serving at scale, especially under unpredictable traffic?</li>
    <li>What metrics does the company use to measure success in infrastructure reliability?</li>
  </ul>

  <p>6. Common Questions:</p>
  <ul>
    <li>‚ÄúDescribe how you‚Äôd architect a scalable model serving system that can handle dynamic batch sizes.‚Äù</li>
    <li>‚ÄúHow would you improve latency if inference spikes 10x during peak usage?‚Äù</li>
    <li>Reddit user mentioned being asked about GPU scheduling tradeoffs in production:
      <ul>
        <li><a href="https://www.reddit.com/r/mlops/comments/example_gpu_serving" target="_blank">reddit /r/mlops</a></li>
      </ul>
    </li>
  </ul>
</div>
